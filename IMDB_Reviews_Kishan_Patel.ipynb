{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9170116",
   "metadata": {},
   "source": [
    "### Project 3: IMDb Review Dataset\n",
    "\n",
    "> **Project by :** Kishan Kanaiyalal Patel  \n",
    "**Student Id  :** 200527734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2873d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from os import path\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer    # Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "import matplotlib.colors\n",
    "\n",
    "import wordcloud  \n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d941be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "\n",
    "df=pd.read_excel('IMDB_Dataset.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249a40d",
   "metadata": {},
   "source": [
    "\n",
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c801f4",
   "metadata": {},
   "source": [
    "\n",
    "> One of the necessary stages we will perform while developing an NLP application is text pre-processing. As humans, we frequently produce text that has several spelling mistakes, short words, unique symbols, emoticons, etc. We can understand this language, but if we want the computer to understand it, we must preprocess it. We'll go over a few of the different kinds of text pre-processing you would need to do when working with text data in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30e405",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f153a",
   "metadata": {},
   "source": [
    "It is the process of changing a word's case to lower case. Our model will treat two words differently if one word (let's say Book) starts the sentence with a capital letter and another word (book) comes later in the phrase without a capital letter. Lowercasing is typically a relatively easy process, and we can use the. lower() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(column):\n",
    "    column = column.str.lower()\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2da797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before applying lower casing: {df['review'][0][:20]}\")\n",
    "\n",
    "df['cleaned_text'] = lowercasing(df['review'])\n",
    "\n",
    "print(f\"After applying lower casing : {df['cleaned_text'][0][:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec2e08",
   "metadata": {},
   "source": [
    "\n",
    "### Removing HTML Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b2021",
   "metadata": {},
   "source": [
    "HTML tags such as header, body, anchor, etc. will always be present while web scraping. These tags shouldn't be used because they won't improve the text data we already have. Using regular expressions, these HTML tags can be removed.\n",
    "\n",
    "Our dataset does not seem to be having the HTML tags, but if it is the case, we are performing below mentioned task to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb1be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def html_tag(text):\n",
    "    re_html = re.compile('<.*?>')\n",
    "    return re_html.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1aa4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<h1> This is the first heading in HTML </h1>'\n",
    "print(html_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before removing HTML tags: {df['cleaned_text'][1][:60]}\")\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(html_tag)\n",
    "print(f\"After removing HTML tags : {df['cleaned_text'][1][:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa3c45",
   "metadata": {},
   "source": [
    "### Removing URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee1e2a",
   "metadata": {},
   "source": [
    "Just like HTML tags, URLs are useless for checking the sentiment of reviews, therefore we'll remove them if spotted any in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'My portfolio can be seen at: https://www.kishandigitallab.com/portfolio'\n",
    "def url(text):\n",
    "    re_url = re.compile('https?://\\S+|www\\.\\S+')\n",
    "    return re_url.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec251bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Text before removing URL: {text}')\n",
    "print(f'Text after removing URL : {url(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe939bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the function on the dataset.\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6715b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd42443",
   "metadata": {},
   "source": [
    "### Removing Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b3d1d",
   "metadata": {},
   "source": [
    "Similar to lowercasing, punctuation is often removed because we want the words 'yeah' and 'yeah!' to be handled equally in certain contexts. The term \"can't\" can be translated to \"cant\" and \"can t\" depending on the parameter we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = string.punctuation\n",
    "\n",
    "def punctuations(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Yeah!'\n",
    "print(f'Text before punctuation: {text}')\n",
    "no_punc = punctuations(text)\n",
    "print(f'Text after punctuation : {no_punc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd837201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Text before removing punctuation: {df['cleaned_text'][0]}\\n\")\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(punctuations)\n",
    "print(f\"Text after removing punctuation : {df['cleaned_text'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70deb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defcdea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def textblob_func(text):\n",
    "#     try:\n",
    "#         return TextBlob(text).correct()\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# df['cleaned_text'] = df['cleaned_text'].apply(textblob_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.cleaned_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "\n",
    "# def translate(x):\n",
    "#     blob =TextBlob(x)\n",
    "#     return blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['xxx']=df['cleaned_text'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb788a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.xxx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80413cd",
   "metadata": {},
   "source": [
    "### Removing stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934bdf4",
   "metadata": {},
   "source": [
    "Stop words are words like the, an, so, and that are frequently found in texts but don't offer the model any useful information. By eliminating these words, we may concentrate on the text's more significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "def stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords_english:\n",
    "            continue\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the function on the dataset.\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e33ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(text):\n",
    "    token_list=[]\n",
    "    token_list=re.findall('\\w+',text)\n",
    "    return token_list\n",
    "\n",
    "df['tokened_text']=''\n",
    "for i in range(0,len(df['cleaned_text'])):    \n",
    "    df['tokened_text'].iloc[i]=token(df['cleaned_text'][i].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdcbdb",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895cd7e",
   "metadata": {},
   "source": [
    "Stemming is a process by which we bring the words to their root forms. For e.g. the stem of walking, walks, walked is walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()\n",
    "# def stemmer(text):\n",
    "#     new_text = [stemming.stem(word) for word in text.split()]\n",
    "#     return ' '.join(new_text)\n",
    "\n",
    "def stemmer(list_token):\n",
    "    stemmed_list=[]\n",
    "    for i in list_token:\n",
    "#         print(i)\n",
    "        stemmed_list.append(stemming.stem(i))\n",
    "    return stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed_text'] = df['tokened_text'].apply(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b5bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66ca7c",
   "metadata": {},
   "source": [
    "### Splitting large dataset in smaller one "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd01647",
   "metadata": {},
   "source": [
    "Since 25,000 can take so much of time, I have reduced the dataset to 6000 points, having equal ratio of positive reviews and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling data with balance\n",
    "    # Total sample: 25k points\n",
    "    # 3k points -> class 0\n",
    "    # 3k points -> class 1\n",
    "\n",
    "negative_samples = df[df[\"sentiment\"] == 0].sample(n=3000, random_state=60)\n",
    "positive_samples = df[df[\"sentiment\"] == 1].sample(n=3000, random_state=60)\n",
    "\n",
    "# Merge and shuffle the imbalanced data\n",
    "reduced_df = pd.concat([negative_samples, positive_samples]).sample(frac=1, random_state=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672619a1",
   "metadata": {},
   "source": [
    "\n",
    "## TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b13796",
   "metadata": {},
   "source": [
    "According to the TF-IDF, a term's relevance is inversely proportional to how frequently it appears in various documents. A term's frequency in a document is revealed by TF, while its relative rarity within the corpus of texts is revealed by IDF. We can determine our final TF-IDF value by multiplying these numbers collectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer = stemmer)\n",
    "X_tfidf = tfidf.fit_transform(reduced_df['tokened_text'])\n",
    "print(X_tfidf.shape)\n",
    "print(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8ef59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "tfidf_df.columns = tfidf.get_feature_names()\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)\n",
    "\n",
    "reduced_df['body_len'] = reduced_df['review'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "reduced_df['punct%'] = reduced_df['review'].apply(lambda x: count_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d15d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = pd.concat([reduced_df['body_len'], reduced_df['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc789bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
